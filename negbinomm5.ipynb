{"cells":[{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# !pip install --upgrade pip\n# !pip install swifter","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nfrom dask_ml import preprocessing\nfrom datetime import datetime, timedelta\n\nfrom patsy import dmatrices\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport swifter\n\nimport os, sys, gc, time, warnings, pickle, psutil, random\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DECLARE CONSTANTS\nNUM_ITEMS = 30490 # 3490 prods * 10 stores\nnrows = int(365 * 2 * NUM_ITEMS)\nFIRST_DAY = 350\nh = 28 \nmax_lags = 57\ntr_last = 1913\nfday = datetime(2016,4, 25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# simple function to read the data in the competition files\ndef load_sub(PATH='/kaggle/input/', submission_only=False):\n    print('Reading files...')\n    if submission_only:\n        sample_submission = pd.read_csv(PATH+'m5-forecasting-accuracy/sample_submission.csv').pipe(reduce_mem_usage)\n        return sample_submission\n    \ndef cal_transform(df):\n    sc = MinMaxScaler(feature_range=(0, 1))\n    cols = ['wholesale_inventory','wholesale_sales','retai_t_f_sales','urban_cpi','cpi','unemp_rate','ppi','gdp']\n    df_scaled = sc.fit_transform(df[cols])\n    df_scaled = pd.DataFrame(df_scaled)\n    for i, col in enumerate(cols):\n        df[col] = df_scaled[i]\n    return df\n\ndef create_dt(is_train = True, nrows = None, first_day = 1200, PATH='/kaggle/input/'):\n    prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\", dtype = PRICE_DTYPES)\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            prices[col] = prices[col].cat.codes.astype(\"int16\")\n            prices[col] -= prices[col].min()\n            \n    cal = pd.read_csv(PATH+'m5-cal-mod2/calendar_mod_2.csv', dtype = CAL_DTYPES)\n    cal = cal_transform(cal).pipe(reduce_mem_usage)\n\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    \n    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    dt = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_validation.csv\", \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n    \n    for col in catcols:\n        if col != \"id\":\n            dt[col] = dt[col].cat.codes.astype(\"int16\")\n            dt[col] -= dt[col].min()\n    \n    if not is_train:\n        for day in range(tr_last+1, tr_last+ 28 +1):\n            dt[f\"d_{day}\"] = np.nan\n    \n    dt = pd.melt(dt,\n                  id_vars = catcols,\n                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    dt = dt.merge(cal, on= \"d\", copy = False)\n    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    \n    dt = dt.assign(d = dt.d.str[2:].astype(int))\n    dt = reduce_mem_usage(dt)\n    \n    return dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_fea(dt):\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 28]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n\n    date_features = {\n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n    }\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in dt.columns:\n            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n        else:\n            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")\n            \n    dt = reduce_mem_usage(dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nnew_fe = False\nif new_fe:\n    df = create_dt(is_train=True, first_day= FIRST_DAY)\n    create_fea(df)\n    print('Saving to Pickle...')\n    df.to_pickle('df.pkl')\nelse:\n    df = pd.read_pickle('df.pkl')\n    df = reduce_mem_usage(df)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############ Truncating data set for fast testing #############\n\n############ 90 days training\n############ 28 day test period, \n############ 118 total days in dataset\n############ Validation same as test set for testing purposes\n##############################################################\ndf = df[(df.d > (1913 - 118)) & (df.d <= 1913)].reset_index(drop=True)\ndf.dropna(inplace = True)\ngc.collect()\n\n####################### Masks for data #######################\ntrain_mask = df['d']<=(1913-28)\n\n# Test mask, also used here as validation\ntest_mask = df['d']>(1913-28)\n\n################### Feature columns ########################\nkeep_fe = ['cat_id', 'state_id','sales', 'wday', 'month','event_name_1', 'event_name_2', 'wholesale_inventory','wholesale_sales', 'retai_t_f_sales', 'urban_cpi', 'cpi', 'unemp_rate',\n       'ppi', 'gdp', 'sell_price', 'lag_7', 'lag_28', 'rmean_7_7','rmean_28_7', 'rmean_7_28', 'rmean_28_28']\n# remove_features = [col for col in df.columns if col not in keep_fe]\n# features_columns = [col for col in list(df) if col not in remove_features]\n\ndf_test = df[test_mask][keep_fe]\ndf_test = df_test.dropna()\n\ndf_train = df[train_mask][keep_fe]\ndf_train = df_train.dropna()\n\n# We also need the test_id for submission function \ntest_id = df[test_mask][['id', 'd']].reset_index(drop=True)\n\nprint('Training data set length='+str(df_train.shape))\nprint('Testing data set length='+str(df_test.shape))\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setup the regression expression in patsy notation. \n#We are telling patsy that sales is our dependent variable and it depends on the other regression variables\nexpr = \"\"\"sales ~ cat_id + state_id + wday + month + event_name_1 + event_name_2 + wholesale_inventory + wholesale_sales + retai_t_f_sales + urban_cpi + cpi + unemp_rate + ppi + gdp + sell_price + lag_7 + lag_28 + rmean_7_7 + rmean_28_7 + rmean_7_28 + rmean_28_28\"\"\"\n\n#Set up the X and y matrices for the training and testing data sets\ny_train, X_train = dmatrices(expr, df_train) #, return_type='dataframe'\ny_test, X_test = dmatrices(expr, df_test) #, return_type='dataframe'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lambda(df_train, y_train, X_train):\n    #Using the statsmodels GLM class, train the Poisson regression model on the training data set\n    poisson_training_results = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()\n    \n    #print out the training summary\n    print(poisson_training_results.summary())\n    \n    #Add the Î» vector as a new column called 'LAMBDA' to the Data Frame of the training data set\n    df_train['LAMBDA'] = poisson_training_results.mu\n    \n    #auxiliary OLS regression add a derived column called auxiliary OLS regression'AUX_OLS_DEP' to the pandas Data Frame. This new column will store the values of the dependent variable of the OLS regression\n    df_train['AUX_OLS_DEP'] = df_train.swifter.apply(lambda x: ((x['sales'] - x['LAMBDA'])**2 - x['sales']) / x['LAMBDA'], axis=1)\n    \n    return df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf_train = get_lambda(df_train, y_train, X_train)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Saving to Pickle...')\ndf_train.to_pickle('df_train.pkl')\n# df_train = pd.read_pickle('df_train.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ols_aux(df_train, y_train, X_train):\n    #use patsy to form the model specification for the OLSR\n    ols_expr = \"\"\"AUX_OLS_DEP ~ LAMBDA - 1\"\"\"\n    \n    #Configure and fit the OLSR model\n    aux_olsr_results = smf.ols(ols_expr, df_train).fit()\n    alpha = aux_olsr_results.params[0]\n    return alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nalpha = get_ols_aux(df_train, y_train, X_train)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_test, df_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_nbm(y_train, X_train, alpha):\n    #train the NB2 model on the training data set\n    nb2_training = sm.GLM(y_train, X_train,family=sm.families.NegativeBinomial(alpha=alpha))\n    gc.collect()\n    nb2_training_results = nb2_training.fit()\n    #print the training summary\n    print(nb2_training_results.summary())\n    \n    return nb2_training_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nnb2_training_results = get_nbm(y_train, X_train, alpha)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_preds(nb2_training_results, X_test, test_id):\n    #make some predictions using our trained NB2 model\n    nb2_predictions = nb2_training_results.get_prediction(X_test)\n    predictions_summary_frame = nb2_predictions.summary_frame()\n    y_pred = predictions_summary_frame['mean']\n    test_id['sales'] = y_pred.values\n    y_true = y_test['sales']\n    \n    submission = load_sub(submission_only=True)\n    \n    predictions = test_id[['id', 'd', 'sales']]\n    predictions = pd.pivot(predictions, index = 'id', columns = 'd', values = 'sales').reset_index()\n    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\n    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n    evaluation = submission[submission['id'].isin(evaluation_rows)]\n\n    validation = submission[['id']].merge(predictions, on = 'id')\n    final = pd.concat([validation, evaluation])\n    final.to_csv('submission.csv', index = False)\n    del submission, validation, evaluation, predictions\n    gc.collect\n    return final, y_pred, y_true","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfinal, y_pred, y_true = make_preds(nb2_training_results, X_test, test_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(40,20))\nfig.suptitle('Predicted versus actual')\npredicted, = plt.plot(X_test.index, y_pred, 'g--', label='Predicted sales')\nactual, = plt.plot(X_test.index, y_true, 'r--', label='Actual sales')\nplt.legend(handles=[predicted, actual])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}